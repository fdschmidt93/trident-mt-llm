dataset:
  _target_: datasets.load.load_dataset

  _convert_: all
  path: parquet
  split: train
  data_files: ${hydra:runtime.cwd}/data/datasets/fineweb/sample/10BT/*.parquet
  num_proc: 16

preprocessing:
  method:
    map:
      batched: true
      num_proc: 32
      function:
        _target_: src.mt_llm.processing.preprocess_alignment
        _partial_: True
        source_tokenizer:
          _target_: transformers.AutoTokenizer.from_pretrained
          pretrained_model_name_or_path: ${run.base_model}
        target_tokenizer:
          _target_: transformers.AutoTokenizer.from_pretrained
          pretrained_model_name_or_path: ${module.model.nllb.pretrained_model_name_or_path}
          src_lang: eng_Latn
        tokenize_kwargs:
          truncation: True
          max_length: 512
          return_attention_mask: True
  apply:
    split_if_multi_gpu:
      _target_: src.mt_llm.utils.split_by_node_if_multi_gpu
      world_size: ${trainer.devices}

dataloader:
  _convert_: all
  _target_: torch.utils.data.dataloader.DataLoader
  collate_fn:
    _target_: src.mt_llm.processing.DataCollatorForTokenAlignedDistillation
    llm_tokenizer:
      _target_: transformers.AutoTokenizer.from_pretrained
      pretrained_model_name_or_path: ${oc.select:run.base_model,${run.pretrained_model_name_or_path}}
      padding_side: right
    nllb_tokenizer:
      _target_: transformers.AutoTokenizer.from_pretrained
      pretrained_model_name_or_path: ${module.model.nllb.pretrained_model_name_or_path}
      src_lang: eng_Latn
      padding_side: right
    tokenize_kwargs:
      padding: True
      max_length: 512
      return_attention_mask: True
      return_tensors: pt
  batch_size: ${run.train_batch_size} # copied into all train, val, test
  pin_memory: true # copied into all train, val, test
  shuffle: true # will be copied in to val and test
  num_workers: 4
