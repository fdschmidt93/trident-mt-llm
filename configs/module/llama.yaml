defaults:
  - default

model:
  _target_: peft.get_peft_model
  model:
    _target_: peft.prepare_model_for_kbit_training
    use_gradient_checkpointing: false
    model:
      _target_: transformers.AutoModel.from_pretrained
      pretrained_model_name_or_path: ${run.pretrained_model_name_or_path}
      pad_token_id: 128001
      device_map: "cuda:0"
      _attn_implementation: sdpa
      quantization_config: 
        _target_: transformers.BitsAndBytesConfig
        load_in_4bit: True
        bnb_4bit_quant_type: "nf4"
        bnb_4bit_use_double_quant: True
        bnb_4bit_compute_dtype:
          _target_: src.mt_llm.utils.get_torch_dtype
          type_: bfloat16
  peft_config:
    _target_: peft.LoraConfig
    r: 16
    lora_alpha: 32
    target_modules: all-linear
    lora_dropout: 0.05 
    bias: "none" 
    task_type: "FEATURE_EXTRACTION"

optimizer:
  _target_: bitsandbytes.optim.AdamW8bit
