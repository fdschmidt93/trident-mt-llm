defaults:
  - default

nllb_ckpt: null
# pooling_strategy: eos
pooling_strategy: mean
model:
  _target_: src.mt_llm.module.NLLBLlamaEncoder
  nllb:
    _target_: transformers.AutoModel.from_pretrained
    pretrained_model_name_or_path: facebook/nllb-200-distilled-600M
    torch_dtype:
      _target_: src.mt_llm.utils.get_torch_dtype
      type_: bfloat16
  llama:
    _target_: peft.get_peft_model
    model:
      _target_: transformers.AutoModel.from_pretrained
      pretrained_model_name_or_path: ${run.pretrained_model_name_or_path}
      config:
        _target_: transformers.AutoConfig.from_pretrained
        pretrained_model_name_or_path: ${oc.select:run.base_model,${run.pretrained_model_name_or_path}}
        _attn_implementation: flash_attention_2
      device_map: "cuda"
      torch_dtype:
        _target_: src.mt_llm.utils.get_torch_dtype
        type_: bfloat16
    peft_config:
      _target_: peft.LoraConfig
      r: 16
      lora_alpha: 32
      target_modules: all-linear
      lora_dropout: 0.05 
      bias: "none" 
      task_type: "FEATURE_EXTRACTION"

optimizer:
  _target_: bitsandbytes.optim.AdamW8bit
