# @package _global_

defaults:
  - /dataspecs@datamodule.train: nusax_train_llm2vec
  - /dataspecs@datamodule.val:
      - nusax_val_test_en
  - /dataspecs/module@datamodule.val:
      - nusax_val_test_en_nllb-llm2vec
  - default_llm2vec
  - override /module: nllb-llm2vec

run:
  task: nusax_llm2vec_train
  pretrained_model_name_or_path: ${hydra:runtime.cwd}/data/model/llm2vec/
  base_model: "McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp"
  seed: 42
  epoch: 8
  train_batch_size: 16
  val_test_batch_size: 128
  max_length: 256

trainer:
  num_sanity_val_steps: 0
  max_epochs: 20
  accumulate_grad_batches: 2
  check_val_every_n_epoch: 2
  enable_checkpointing: false

module:
  _target_: src.mt_llm.module.AutoModuleForSequenceClassificationDistillation
  save_checkpoint_on_validation_dir: "${hydra:runtime.output_dir}/checkpoints/"
  ckpt: ${hydra:runtime.cwd}/logs/mt-llm/llm2vec/nusax/seed-${run.seed}/checkpoints/validation-epoch=${run.epoch}.ckpt
  optimizer:
    lr: 1e-4
  scheduler:
    num_warmup_steps: 0.1

datamodule:
  train:
    nusax:
      preprocessing:
        apply:
          convert:
            _target_: src.mt_llm.processing.EmbeddedDataset
            key: "sequence_embeds"
            tensors:
              _target_: torch.load
              _args_:
                - "/gpfs/bwfor/work/ws/ma_fabiasch-tx/trident-mt-llm/nusax/embeds_seed-${run.seed}_epoch-${run.epoch}.pth"

logger:
  wandb:
    name: "steps=${oc.select:run.steps,10000}_model=distill-${run.base_model}_epochs=${trainer.max_epochs}_batch-size=${_log_vars.train_batch_size}_accumulate-grad-batches=${trainer.accumulate_grad_batches}_lr=${module.optimizer.lr}_scheduler=${module.scheduler.num_warmup_steps}_seed=${run.seed}"
    tags:
      - "${run.base_model}"
      - "bs=${_log_vars.train_batch_size}"
      - "lr=${module.optimizer.lr}"
      - "scheduler=${module.scheduler.num_warmup_steps}"
      - "accumulate-grad-batches=${trainer.accumulate_grad_batches}"
    entity: wuenlp
    project: ${run.task}

callbacks:
  learning_rate:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
