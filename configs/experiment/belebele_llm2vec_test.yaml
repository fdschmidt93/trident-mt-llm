# @package _global_

defaults:
  - /dataspecs@datamodule.train: belebele_train_llm2vec
  - /dataspecs@datamodule.test:
      belebele_train_llm2vec # - belebele_val_test - belebele_val_test_translate_test_600M
      # - belebele_val_test_translate_test_3B
  # - /dataspecs/module@datamodule.test:
      # - belebele_val_test_llm2vec
      # - belebele_val_test_translate_test_600M_llm2vec
      # - belebele_val_test_translate_test_3B_llm2vec
  - default_llm2vec

run:
  task: belebele_llm2vec_test
  pretrained_model_name_or_path: ${hydra:runtime.cwd}/data/model/llm2vec/
  epoch: 9
  checkpoint: ${hydra:runtime.cwd}/logs/mt-llm/llm2vec/belebele/seed-${run.seed}/checkpoints/validation-epoch=${run.epoch}.ckpt
  seed: 42
  train_batch_size: 32
  val_test_batch_size: 32
  max_length: 4096

trainer:
  num_sanity_val_steps: 0
  max_epochs: 5
  accumulate_grad_batches: 16
  val_check_interval: 0.5
  limit_train_batches: 0
  limit_test_batches: 1.0

module:
  _target_: src.mt_llm.module.AutoModuleForMultipleChoice.load_from_checkpoint
  save_checkpoint_on_validation_dir: "${hydra:runtime.output_dir}/checkpoints/"
  optimizer:
    lr: 1e-4
  scheduler:
    num_warmup_steps: 0.1
  checkpoint_path: ${run.checkpoint}

logger: null
  # csv:
  #   _target_: lightning.pytorch.loggers.CSVLogger
  #   save_dir: ${hydra:runtime.output_dir}
  # wandb:
  #   name: "model=${run.base_model}_epochs=${trainer.max_epochs}_batch-size=${_log_vars.train_batch_size}_accumulate-grad-batches=${trainer.accumulate_grad_batches}_lr=${module.optimizer.lr}_scheduler=${module.scheduler.num_warmup_steps}_seed=${run.seed}_ckpt-epoch=${run.epoch}"
  #   tags:
  #     - "${run.base_model}"
  #     - "bs=${_log_vars.train_batch_size}"
  #     - "lr=${module.optimizer.lr}"
  #     - "scheduler=${module.scheduler.num_warmup_steps}"
  #     - "accumulate-grad-batches=${trainer.accumulate_grad_batches}"
  #   entity: wuenlp
  #   project: ${run.task}

callbacks:
  learning_rate:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
